---
title: raft源码解读
date: 2017-11-27 14:07:13
tags:
---
以下是 hashicorp/raft 1.0 第一次源码阅读后的整理。根据经验，最开始往往有理解不正确的地方，后续加深理解后再整理修正，并与当前对比

## 1. Apply过程
### 1.1 leader
- api.Apply 写logfuture到r.applyCh
- raft.leaderLoop 读applyCh然后调用 dispatchLogs
- raft.dispatchLogs 先写入到leaderStare的inflight,再持久化到logStore，然后通知commit(commitCh)，最后通知复制log(triggerCh)
- 先说commit链路。raft.leaderLoop处理commit，循环从leaderState中读出每个待提交的LogFuture,然后调用r.processLogs
- raft.processLogs循环调用processLog，里面向fsmMutateCh写入构造的commitTuple消息
- fsm.runFSM读fsmMutateCh，执行commit函数，这个函数里面调用fsm对象的Apply方法，这个方法需要由用户自定义实现。
- 再回到复制链路。启动时，每个follower都创建1个goroutine，运行r.replicate函数
- 获得lastIndex，构造tranReq，r.setupAppendEntries从logstore读出需复制的log
- r.trans.AppendEntries 执行RPC
- commit补充。leader被通知commitCh，是当大多数follower复制成功消息后才被通知提交的。
- 复制链路有个commitTimeout(默认50ms), 定期把当前的commitIndex等信息通过RPC传递到Follower。Command类型还是AppendEntriesRequest

### 1.2 Follower
- raft.runFollower 读r.rpcCh，然后调用processRPC处理
- 检查rpc的类型，处理AppendEntriesRequest
- 检查一大堆，然后先持久化到logStore，再commit。
- commit过程与leader类似，调用processLogs进行处理。

## 2. Term变化过程
- 先进入到 Candidate 状态
- 选举自己。发送投票请求给其它节点，请求包括currentTerm，lastTerm,lastIndex.
- 优先投票给currentTerm，lastTerm，lastIndex最大的node

## 3. Node变化过程
- 添加节点。api.AddVoter必须在leader执行. 先更新configurations，接着调用dispatchLogs;
调用startStopReplication 启动新节点的复制goroutine。
- 删除节点。api.RemoveServer也必须在leader执行。流程和添加节点类似，commandType 变成RemoveServer。
删除leader节点，触发重新选举。处理完log提交后，修改状态 stepdown 为true，然后关闭或者变成Follower。
- 节点故障。 Follower的心跳超时，改变状态为 Candidate 重新选举。

## 4. snapshot 过程
- r.runSnapshots 默认每隔120s 检查一次上次snapshot的index和当前最新的index，二者差值超过8192，执行snapshot
- 先创建snapshot的sink，然后持久化，再接着compactLog(删除已经snapshot的log)
- 无论leader还是Follower都启动snapshot的goroutine

## 5. restore 过程
- 新加入的节点，状态为Fllower
- leader主动同步log到新节点
- 构造复制请求时，发现需要同步很久以前的log，且当前logstore不存在该log
- 此时发起installSnapshot的请求给Follower，取消同步log的请求
- follwer接收snapshot并应用到本地
- leader的周期同步commit的机制逐步把最近的log同步给Follower
